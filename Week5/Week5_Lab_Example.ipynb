{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab for Week 5: Fitting a Feed Forward Neural Network\n",
    "\n",
    "In Milestone 1 you chose a project topic you are interested in and found some datasets you could use. You also already built a [data loader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Today, we will fit a baseline feed forward neural network to one of the datasets you chose by following the pytorch tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a Data Loader\n",
    "You are encouraged to use one of the data loaders you built for milestone 1. Otherwise use the one described [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Tip: If your targets are labels, you might want to use the lambda transformation described [here](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html) to turn your target into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Model\n",
    "Follow [this tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) to define a model. Make sure that the input dimension fits the dimensionality of your data, and the output dimension fits the dimensionality of your targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Model\n",
    "\n",
    "Follow the tutorial [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Runs one full training epoch on the given model using the provided dataloader.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of training data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be trained.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the model parameters.\n",
    "\n",
    "    Behavior:\n",
    "        - Sets the model to training mode (important for layers like dropout and batch normalization).\n",
    "        - Iterates over the data batches:\n",
    "            - Computes the model's predictions.\n",
    "            - Calculates the loss.\n",
    "            - Performs backpropagation and updates model weights.\n",
    "        - Prints progress every 100 batches, showing the current loss and the number of samples processed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of test data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "\n",
    "    Behavior:\n",
    "        - Sets the model to evaluation mode (which disables behaviors like dropout).\n",
    "        - Iterates over the test data without computing gradients (using torch.no_grad() for efficiency).\n",
    "        - Accumulates the total loss and counts the number of correct predictions.\n",
    "        - Computes the average loss and overall accuracy.\n",
    "        - Prints the test accuracy and average loss.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\" \n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.307856  [   64/60000]\n",
      "loss: 2.284600  [ 6464/60000]\n",
      "loss: 2.273957  [12864/60000]\n",
      "loss: 2.258824  [19264/60000]\n",
      "loss: 2.248000  [25664/60000]\n",
      "loss: 2.228622  [32064/60000]\n",
      "loss: 2.229075  [38464/60000]\n",
      "loss: 2.207806  [44864/60000]\n",
      "loss: 2.193921  [51264/60000]\n",
      "loss: 2.158929  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 2.157776 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.174810  [   64/60000]\n",
      "loss: 2.160250  [ 6464/60000]\n",
      "loss: 2.105361  [12864/60000]\n",
      "loss: 2.106467  [19264/60000]\n",
      "loss: 2.081221  [25664/60000]\n",
      "loss: 2.017720  [32064/60000]\n",
      "loss: 2.044077  [38464/60000]\n",
      "loss: 1.974459  [44864/60000]\n",
      "loss: 1.964731  [51264/60000]\n",
      "loss: 1.892145  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.893542 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.931175  [   64/60000]\n",
      "loss: 1.905419  [ 6464/60000]\n",
      "loss: 1.784084  [12864/60000]\n",
      "loss: 1.810077  [19264/60000]\n",
      "loss: 1.733045  [25664/60000]\n",
      "loss: 1.670145  [32064/60000]\n",
      "loss: 1.697738  [38464/60000]\n",
      "loss: 1.597025  [44864/60000]\n",
      "loss: 1.616099  [51264/60000]\n",
      "loss: 1.507498  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.524493 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.592452  [   64/60000]\n",
      "loss: 1.563825  [ 6464/60000]\n",
      "loss: 1.406238  [12864/60000]\n",
      "loss: 1.472373  [19264/60000]\n",
      "loss: 1.367390  [25664/60000]\n",
      "loss: 1.352375  [32064/60000]\n",
      "loss: 1.378210  [38464/60000]\n",
      "loss: 1.296534  [44864/60000]\n",
      "loss: 1.333008  [51264/60000]\n",
      "loss: 1.225403  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.254560 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.332302  [   64/60000]\n",
      "loss: 1.320270  [ 6464/60000]\n",
      "loss: 1.149820  [12864/60000]\n",
      "loss: 1.251343  [19264/60000]\n",
      "loss: 1.126341  [25664/60000]\n",
      "loss: 1.150539  [32064/60000]\n",
      "loss: 1.180381  [38464/60000]\n",
      "loss: 1.111532  [44864/60000]\n",
      "loss: 1.154964  [51264/60000]\n",
      "loss: 1.062758  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.086457 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 3 different hyper parameter settings (e.g. different learning rates, different loss functions that make sense for your data, etc.)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Record results\n",
    "\n",
    "Results are typically recorded in a table. For inspiration, check out the famous [Attention is All You Need paper](https://arxiv.org/abs/1706.03762). This paper first introduced the transformer architecture we will learn about later this semseter and has been highly influential in deep learnin. Check out how results are reported in Tables 2, 3, and 4.\n",
    "Note that because here the transformers are used for text generation results are reported using the BLEU score (the higher the better). \n",
    "\n",
    "You should create your own result tables to record how different hyperparameter settings affect performance. Make sure to record test accuracy, not traingin accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLHEP3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
